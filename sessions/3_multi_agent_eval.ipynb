{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1efcdf",
   "metadata": {},
   "source": [
    "# Multi-Agent System Evaluation for Financial Analysis\n",
    "\n",
    "## Overview\n",
    "This is a multi-agent financial analysis system built with LangGraph that intelligently routes user queries between internal proprietary research and external web search.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "The agent uses a **routing-based architecture** to determine the best information source:\n",
    "\n",
    "- **Internal Research Route (RAG)**: When users ask about \"our research\", \"proprietary analysis\", or \"internal findings\"\n",
    "- **External Search Route (Tavily)**: For general market questions, current stock prices, or public information\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. Router Node\n",
    "- Analyzes incoming queries for internal research keywords\n",
    "- Routes to either RAG pipeline or Tavily search\n",
    "\n",
    "### 2. RAG Pipeline\n",
    "- **Retrieval**: Searches internal PDF documents (AMD and NVIDIA analysis)\n",
    "- **Quality Check**: Evaluates if retrieved docs can answer the query\n",
    "- **Generation**: Creates response based on internal research\n",
    "- **Fallback**: Falls back to Tavily if internal docs insufficient\n",
    "\n",
    "### 3. Tavily Search\n",
    "- Performs web search for external information\n",
    "- Generates responses based on current market data\n",
    "\n",
    "## Data Sources\n",
    "- **Internal**: AMD and NVIDIA proprietary analysis PDFs\n",
    "- **External**: Web search via Tavily API\n",
    "\n",
    "## Response Format\n",
    "- Maximum 3 sentences per response\n",
    "- Clear attribution of information sources\n",
    "- Honest acknowledgment when information isn't available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c03ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langsmith langchain-openai langchain-community openai openevals PyMuPDF langchain-tavily langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up environment variables with your input\n",
    "print(\"Please enter your API keys to get started:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# LangSmith tracing setting\n",
    "langsmith_tracing = input(\"Enable LangSmith tracing? (true/false) [default: true]: \").strip() or \"true\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = langsmith_tracing\n",
    "\n",
    "# LangSmith API key (secure input)\n",
    "if not os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    langsmith_api_key = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = langsmith_api_key\n",
    "else:\n",
    "    print(\"✓ LangSmith API key already set\")\n",
    "\n",
    "# OpenAI API key (secure input)\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    openai_api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "else:\n",
    "    print(\"✓ OpenAI API key already set\")\n",
    "\n",
    "# Tavily API key (secure input)\n",
    "if not os.getenv(\"TAVILY_API_KEY\"):\n",
    "    tavily_api_key = getpass.getpass(\"Enter your Tavily API key: \")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
    "else:\n",
    "    print(\"✓ Tavily API key already set\")\n",
    "\n",
    "print(\"\\n✓ Environment setup complete!\")\n",
    "print(\"You can now proceed with the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "from langsmith import Client\n",
    "from typing import TypedDict, Annotated\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b877d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    query: str\n",
    "    retrieved_docs: str\n",
    "    has_sufficient_info: bool\n",
    "    response: str\n",
    "\n",
    "# Initialize models and tools\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "tavily_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF files to load\n",
    "pdf_files = [\n",
    "    \"/content/AMD_INTERNAL_ANALYSIS_20250724.pdf\",\n",
    "    \"/content/NVIDIA_INTERNAL_ANALYSIS_20250724.pdf\"\n",
    "]\n",
    "\n",
    "# Load documents from the URLs\n",
    "docs = [PyMuPDFLoader(pdf_file).load() for pdf_file in pdf_files]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add the document chunks to the \"vector store\" using OpenAIEmbeddings\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# With langchain we can easily turn any vector store into a retrieval component:\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router Node\n",
    "def router_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"Route to RAG if query mentions internal research, otherwise to Tavily\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # Check if query is about internal research\n",
    "    internal_keywords = [\"internal research\", \"our research\", \"proprietary\", \"our study\", \n",
    "                        \"our analysis\", \"internal study\", \"our findings\", \"our team\"]\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    is_internal = any(keyword in query_lower for keyword in internal_keywords)\n",
    "    \n",
    "    # Route decision\n",
    "    route = \"RAG\" if is_internal else \"TAVILY\"\n",
    "    \n",
    "    state[\"messages\"].append({\"role\": \"system\", \"content\": f\"Routing to: {route}\"})\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a61a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Retrieval Node\n",
    "def rag_retrieval_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"Retrieve relevant documents from internal research\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    \n",
    "    # Format retrieved content\n",
    "    if relevant_docs:\n",
    "        retrieved_content = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        state[\"retrieved_docs\"] = retrieved_content\n",
    "    else:\n",
    "        state[\"retrieved_docs\"] = \"\"\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22617076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Quality Check Node\n",
    "def rag_check_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"Check if retrieved documents have sufficient information\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        state[\"has_sufficient_info\"] = False\n",
    "        return state\n",
    "    \n",
    "    # Ask LLM to evaluate if retrieved docs can answer the query\n",
    "    check_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    \n",
    "    Retrieved Information:\n",
    "    {retrieved_docs}\n",
    "    \n",
    "    Can the retrieved information sufficiently answer the user's query? \n",
    "    Respond with only 'YES' or 'NO'.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(check_prompt)\n",
    "    state[\"has_sufficient_info\"] = \"YES\" in response.content.upper()\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ac049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Generation Node\n",
    "def rag_generation_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"Generate response based on retrieved documents\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "    \n",
    "    generation_prompt = f\"\"\"\n",
    "    Based on our internal research, answer the following query:\n",
    "    \n",
    "    Internal Research Data:\n",
    "    {retrieved_docs}\n",
    "    \n",
    "    User Query: {query}\n",
    "    \n",
    "    Provide a comprehensive answer based on the internal research provided. If you don't see the answer in the internal research, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(generation_prompt)\n",
    "    state[\"response\"] = response.content\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d21b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tavily_search_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"Search the web using Tavily and generate response\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # Perform web search\n",
    "    search_results = tavily_search.invoke({\"query\": query})\n",
    "    \n",
    "    if not search_results:\n",
    "        state[\"response\"] = \"No search results found.\"\n",
    "        return state\n",
    "\n",
    "    formatted_results = []\n",
    "    results_list = search_results.get('results', []) if isinstance(search_results, dict) else []\n",
    "    for result in results_list[:3]:\n",
    "        formatted_results.append(f\"- {result.get('content', '')}\")\n",
    "        \n",
    "    # Generate response based on search results\n",
    "    search_prompt = f\"\"\"\n",
    "    Based on the following web search results, answer the user's query:\n",
    "    \n",
    "    Search Results:\n",
    "    {formatted_results}\n",
    "    \n",
    "    User Query: {query}\n",
    "    \n",
    "    Provide an answer based on the search results. If you don't see the answer in the search results, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(search_prompt)\n",
    "    state[\"response\"] = response.content\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809aa28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "def create_agent():\n",
    "    \"\"\"Create the LangGraph workflow\"\"\"\n",
    "    graph_builder = StateGraph(GraphState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph_builder.add_node(\"router\", router_node)\n",
    "    graph_builder.add_node(\"rag_retrieval\", rag_retrieval_node)\n",
    "    graph_builder.add_node(\"rag_check\", rag_check_node)\n",
    "    graph_builder.add_node(\"rag_generation\", rag_generation_node)\n",
    "    graph_builder.add_node(\"tavily_search\", tavily_search_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    graph_builder.set_entry_point(\"router\")\n",
    "    \n",
    "    # Add routing from router node\n",
    "    def route_query(state):\n",
    "        query_lower = state[\"query\"].lower()\n",
    "        internal_keywords = [\"internal research\", \"our research\", \"proprietary\", \"our study\", \n",
    "                            \"our analysis\", \"internal study\", \"our findings\", \"our team\"]\n",
    "        \n",
    "        if any(keyword in query_lower for keyword in internal_keywords):\n",
    "            return \"rag\"\n",
    "        return \"tavily\"\n",
    "    \n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_query,\n",
    "        {\n",
    "            \"rag\": \"rag_retrieval\",\n",
    "            \"tavily\": \"tavily_search\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # RAG flow\n",
    "    graph_builder.add_edge(\"rag_retrieval\", \"rag_check\")\n",
    "    \n",
    "    # Conditional routing from RAG check\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"rag_check\",\n",
    "        lambda state: \"generate\" if state[\"has_sufficient_info\"] else \"tavily\",\n",
    "        {\n",
    "            \"generate\": \"rag_generation\",\n",
    "            \"tavily\": \"tavily_search\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # End states\n",
    "    graph_builder.add_edge(\"rag_generation\", END)\n",
    "    graph_builder.add_edge(\"tavily_search\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    return graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fce3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple execution function\n",
    "def run_agent(query: str):\n",
    "    \"\"\"Execute the agent with a query\"\"\"\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"messages\": [],\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": \"\",\n",
    "        \"has_sufficient_info\": False,\n",
    "        \"response\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    result = app.invoke(initial_state)\n",
    "    \n",
    "    return result[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is NVIDIA's next generation GPU?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = run_agent(query)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"=\" * 80)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413e3d2",
   "metadata": {},
   "source": [
    "# Multi-Agent System Evaluation for Financial Analysis\n",
    "\n",
    "## Why Evaluate Agents?\n",
    "\n",
    "Multi-agent systems have multiple potential points of failure. Systematic evaluation helps us:\n",
    "- **Identify weaknesses** in specific components\n",
    "- **Measure performance** objectively across different scenarios  \n",
    "- **Track improvements** as we iterate on the system\n",
    "- **Build confidence** in production deployment\n",
    "\n",
    "## Our Evaluation Strategy\n",
    "\n",
    "We implement **two complementary evaluation approaches** that test different aspects of the agent:\n",
    "\n",
    "### 1. Intent Routing Evaluation (Single Step)\n",
    "**What it tests**: Does the router correctly identify when to use internal research vs. external search?\n",
    "\n",
    "**Why it matters**: Incorrect routing leads to:\n",
    "- Proprietary questions answered with public information\n",
    "- General questions unnecessarily constrained to internal docs\n",
    "- Poor user experience and potentially wrong answers\n",
    "\n",
    "**How it works**: \n",
    "- Test queries with various phrasings of internal research requests\n",
    "- Measure accuracy of RAG vs. Tavily routing decisions\n",
    "- Focus on edge cases and ambiguous queries\n",
    "\n",
    "### 2. Final Response Evaluation (End-to-End)\n",
    "**What it tests**: Is the final answer accurate?\n",
    "\n",
    "**Why it matters**: Even with correct routing, the agent might:\n",
    "- Hallucinate information not in the source documents\n",
    "- Fail to properly attribute internal vs. external information\n",
    "- Provide responses that are too vague or off-topic\n",
    "\n",
    "**How it works**:\n",
    "- LLM-as-judge evaluation comparing agent responses to ground truth\n",
    "- Tests the complete pipeline from query → routing → retrieval → generation\n",
    "- Evaluates factual accuracy, source attribution, and overall quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0fe2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "# Create dataset for intent routing evaluation\n",
    "intent_examples = [\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"What did our analysts produce for AMD's latest performance?\"},\n",
    "        \"outputs\": {\"route\": \"rag\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"What does our research say about near term catalysts for NVIDIA?\"},\n",
    "        \"outputs\": {\"route\": \"rag\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"What is the latest market news for NVIDIA GPUs?\"},\n",
    "        \"outputs\": {\"route\": \"tavily\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"How is AMD stock performing today?\"},\n",
    "        \"outputs\": {\"route\": \"tavily\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_name_intent = \"Financial Agent: Intent Routing\"\n",
    "if not client.has_dataset(dataset_name=dataset_name_intent):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name_intent)\n",
    "    client.create_examples(\n",
    "        dataset_id=dataset.id,\n",
    "        examples=intent_examples\n",
    "    )\n",
    "\n",
    "# Intent routing evaluator\n",
    "def correct_route(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the agent chose the correct route.\"\"\"\n",
    "    return outputs[\"route\"] == reference_outputs[\"route\"]\n",
    "\n",
    "# Target function for intent routing evaluation\n",
    "def run_intent_classifier(inputs: dict) -> dict:\n",
    "    \"\"\"Run just the routing logic and return the route decision.\"\"\"\n",
    "    query = inputs[\"query\"]\n",
    "    query_lower = query.lower()\n",
    "    internal_keywords = [\"internal research\", \"our research\", \"proprietary\", \"our study\",\n",
    "                        \"our analysis\", \"internal study\", \"our findings\", \"our team\"]\n",
    "    \n",
    "    # Determine route based on keywords\n",
    "    route = \"rag\" if any(keyword in query_lower for keyword in internal_keywords) else \"tavily\"\n",
    "    \n",
    "    return {\"route\": route}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ed064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset for final response evaluation\n",
    "final_response_examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"query\": \"What does our research say about near term catalysts for NVIDIA?\",\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"response\": \"Based on our internal research, NVIDIA's near-term growth will be driven by the production ramp of its new Blackwell architecture chips (B100 and B200) and the resumption of H20 export licenses for the China market. The company is positioned to benefit from the industry's transition from AI training to inference deployment, along with increased enterprise adoption of its AI software platform. Additional momentum should come from sovereign AI initiatives as nations invest in building their own domestic AI infrastructure capabilities.\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"query\": \"What is the latest market news for NVIDIA GPUs?\",\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"response\": \"Strong Demand & Market Dominance: Nvidia continues to be the overwhelming leader in the AI chip market, with its GPUs powering numerous AI applications and data centers. Demand for Nvidia's next-generation Blackwell platform is strong, driven by the increasing complexity of AI models and the emergence of reasoning-based applications.\",\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_name_final = \"Financial Agent: Final Response\"\n",
    "if not client.has_dataset(dataset_name=dataset_name_final):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name_final)\n",
    "    client.create_examples(\n",
    "        dataset_id=dataset.id,\n",
    "        examples=final_response_examples\n",
    "    )\n",
    "\n",
    "# LLM-as-judge evaluator for final responses\n",
    "grader_instructions = \"\"\"You are evaluating a financial analysis agent's responses.\n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the AGENT RESPONSE.\n",
    "\n",
    "Grade the agent response based on:\n",
    "1. Factual accuracy relative to the ground truth\n",
    "2. Appropriateness for a financial analysis context\n",
    "3. Whether it correctly indicates if information comes from internal research vs external sources\n",
    "4. Overall helpfulness and relevance\n",
    "\n",
    "The agent response doesn't need to match exactly, but should be factually consistent and appropriately sourced.\n",
    "\n",
    "Explain your reasoning step-by-step.\"\"\"\n",
    "\n",
    "class Grade(TypedDict):\n",
    "    \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\"\n",
    "    reasoning: Annotated[str, ..., \"Explain your reasoning for whether the actual response is correct or not.\"]\n",
    "    is_correct: Annotated[bool, ..., \"True if the agent response is mostly or exactly correct, otherwise False.\"]\n",
    "\n",
    "# Initialize grader LLM\n",
    "grader_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(Grade, method=\"json_schema\", strict=True)\n",
    "\n",
    "def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\n",
    "    \n",
    "    user = f\"\"\"QUESTION: {inputs['query']}\n",
    "GROUND TRUTH RESPONSE: {reference_outputs['response']}\n",
    "AGENT RESPONSE: {outputs['response']}\"\"\"\n",
    "\n",
    "    grade = grader_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": grader_instructions}, \n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ])\n",
    "    return grade[\"is_correct\"]\n",
    "\n",
    "# Target function for final response evaluation\n",
    "def run_full_agent(inputs: dict) -> dict:\n",
    "    \"\"\"Run the full agent and return the final response.\"\"\"\n",
    "    query = inputs[\"query\"]\n",
    "    \n",
    "    # Use your existing run_agent function\n",
    "    response = run_agent(query)\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f215e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations():\n",
    "    \"\"\"Run both types of evaluations.\"\"\"\n",
    "    \n",
    "    print(\"Running Intent Routing Evaluation...\")\n",
    "    intent_results = client.evaluate(\n",
    "        run_intent_classifier,\n",
    "        data=dataset_name_intent,\n",
    "        evaluators=[correct_route],\n",
    "        experiment_prefix=\"financial-agent-intent-routing\",\n",
    "        max_concurrency=4,\n",
    "    )\n",
    "    \n",
    "    print(\"Intent Routing Results:\")\n",
    "    print(intent_results.to_pandas())\n",
    "    \n",
    "    print(\"\\nRunning Final Response Evaluation...\")\n",
    "    final_results = client.evaluate(\n",
    "        run_full_agent,\n",
    "        data=dataset_name_final,\n",
    "        evaluators=[final_answer_correct],\n",
    "        experiment_prefix=\"financial-agent-final-response\",\n",
    "        num_repetitions=1,\n",
    "        max_concurrency=4,\n",
    "    )\n",
    "    \n",
    "    print(\"Final Response Results:\")\n",
    "    print(final_results.to_pandas())\n",
    "    \n",
    "    return intent_results, final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_results, final_results = run_evaluations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
