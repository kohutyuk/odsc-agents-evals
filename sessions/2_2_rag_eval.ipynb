{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43223c22",
   "metadata": {},
   "source": [
    "# RAG Evaluation for Financial Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "Retrieval Augmented Generation (RAG) enhances Large Language Models by providing relevant external knowledge, making it essential for financial analysis applications. This tutorial focuses on evaluating RAG systems that analyze internal AMD and NVIDIA research and market data.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workshop will teach you to evaluate your financial RAG applications using LangSmith:\n",
    "\n",
    "1. **Create targeted test datasets** with financial questions and expected answers\n",
    "2. **Run your RAG application** against comprehensive evaluation scenarios  \n",
    "3. **Measure performance** using specialized evaluation metrics for RAG systems\n",
    "\n",
    "## RAG Evaluation Workflow\n",
    "\n",
    "Our evaluation process follows three essential steps:\n",
    "\n",
    "### 1. Dataset Creation\n",
    "* Financial questions about AMD and NVIDIA performance, strategy, and market position\n",
    "* Expected answers based on internal analysis and market research\n",
    "* Edge cases covering complex financial scenarios\n",
    "\n",
    "### 2. RAG Application Execution  \n",
    "* Query processing against your internal AMD/NVIDIA knowledge base\n",
    "* Document retrieval from financial reports, analysis, and market data\n",
    "* Answer generation combining retrieved context with LLM reasoning\n",
    "\n",
    "### 3. Multi-Dimensional Evaluation\n",
    "* **Answer relevance:** Does the response address the financial question asked?\n",
    "* **Answer accuracy:** Are the financial facts, figures, and analysis correct?\n",
    "* **Retrieval quality:** Did the system find the most relevant internal documents?\n",
    "\n",
    "## Our Focus\n",
    "For this tutorial, we'll evaluate a financial analysis bot that answers questions about AMD and NVIDIA using your internal research database, ensuring reliable insights for investment and strategic decisions.\n",
    "\n",
    "---\n",
    "\n",
    "*Let's begin by examining how RAG systems handle complex financial queries...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374adeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langsmith langchain-openai langchain-community openai openevals PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up environment variables with your input\n",
    "print(\"Please enter your API keys to get started:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# LangSmith tracing setting\n",
    "langsmith_tracing = input(\"Enable LangSmith tracing? (true/false) [default: true]: \").strip() or \"true\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = langsmith_tracing\n",
    "\n",
    "# LangSmith API key (secure input)\n",
    "if not os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    langsmith_api_key = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = langsmith_api_key\n",
    "else:\n",
    "    print(\"✓ LangSmith API key already set\")\n",
    "\n",
    "# OpenAI API key (secure input)  \n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    openai_api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "else:\n",
    "    print(\"✓ OpenAI API key already set\")\n",
    "\n",
    "print(\"\\n✓ Environment setup complete!\")\n",
    "print(\"You can now proceed with the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6c307",
   "metadata": {},
   "source": [
    "## Create Vector Database\n",
    "\n",
    "We'll build a basic RAG application using your internal financial analysis documents. Our simple implementation follows three core steps:\n",
    "\n",
    "### Internal Financial Data Focus\n",
    "* **Source material:** Internal research reports, market analysis, and strategic assessments\n",
    "\n",
    "### Key Components\n",
    "* **Indexing:** Chunk and embed internal AMD/NVIDIA analysis reports into a vector store\n",
    "* **Retrieval:** Find relevant document chunks based on financial questions  \n",
    "* **Generation:** Combine retrieved context with user questions for LLM processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a40634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF files to load\n",
    "pdf_files = [\n",
    "    \"/content/AMD_INTERNAL_ANALYSIS_20250724.pdf\",\n",
    "    \"/content/NVIDIA_INTERNAL_ANALYSIS_20250724.pdf\"\n",
    "]\n",
    "\n",
    "# Load documents from the URLs\n",
    "docs = [PyMuPDFLoader(pdf_file).load() for pdf_file in pdf_files]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add the document chunks to the \"vector store\" using OpenAIEmbeddings\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With langchain we can easily turn any vector store into a retrieval component:\n",
    "retriever = vectorstore.as_retriever(k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dba52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=1)\n",
    "\n",
    "# Add decorator so this function is traced in LangSmith\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "    # LangChain retriever will be automatically traced\n",
    "    docs = retriever.invoke(question)\n",
    "    docs_string = \"\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    instructions = f\"\"\"You are a finanial analysis assistant who is an expert at analyzing source information and answering questions.\n",
    "    Use the following source documents to answer the user's questions. Only use information from the documents to answer the question.\n",
    "    If you don't see the answer in the documents, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "    Documents:\n",
    "    {docs_string}\"\"\"\n",
    "\n",
    "    # langchain ChatModel will be automatically traced\n",
    "    ai_msg = llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return {\"answer\": ai_msg.content, \"documents\": docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef231d",
   "metadata": {},
   "source": [
    "## Create Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define the examples for the dataset\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What's the most recent fiscal year revenue for AMD?\"},\n",
    "        \"outputs\": {\"answer\": \"In fiscal year 2024, AMD reported revenue of $27.28 billion\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is NVIDIA's market share in the AI chip market?\"},\n",
    "        \"outputs\": {\"answer\": \"NVIDIA dominates the AI chip market with 70-95 percent market share\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Compare the data center business between AMD and NVIDIA\"},\n",
    "        \"outputs\": {\"answer\": \"40B for NVIDIA, 4.5B for AMD\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the dataset and examples in LangSmith\n",
    "dataset_name = \"Finance Analysis RAG\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb2592",
   "metadata": {},
   "source": [
    "## RAG Evaluation Framework\n",
    "\n",
    "We'll evaluate your financial RAG system using four key dimensions. Each evaluator compares different components of the RAG pipeline:\n",
    "\n",
    "### 1. Correctness: Response vs Reference Answer\n",
    "* **Goal:** How accurate is the financial analysis compared to expected answers?\n",
    "* **Requirements:** Ground truth answers in your dataset\n",
    "* **Method:** LLM-as-judge assesses factual accuracy of financial information\n",
    "* **Focus:** Validates correct financial metrics, trends, and company insights\n",
    "\n",
    "### 2. Relevance: Response vs Input Question  \n",
    "* **Goal:** How well does the answer address the original financial question?\n",
    "* **Requirements:** No reference answer needed\n",
    "* **Method:** LLM-as-judge evaluates response helpfulness and directness\n",
    "* **Focus:** Ensures answers stay on-topic for financial queries\n",
    "\n",
    "### 3. Groundedness: Response vs Retrieved Documents\n",
    "* **Goal:** How faithful is the response to the source financial documents?\n",
    "* **Requirements:** No reference answer needed  \n",
    "* **Method:** LLM-as-judge detects hallucinations and unsupported claims\n",
    "* **Focus:** Prevents fabricated financial data or analysis\n",
    "\n",
    "### 4. Retrieval Relevance: Retrieved Docs vs Input Question\n",
    "* **Goal:** How relevant are the retrieved documents for answering the question?\n",
    "* **Requirements:** No reference answer needed\n",
    "* **Method:** LLM-as-judge assesses document-question alignment  \n",
    "* **Focus:** Validates that the right financial reports were found\n",
    "\n",
    "### Evaluation Strategy\n",
    "Each metric provides unique insights into your RAG system's performance, helping identify whether issues stem from retrieval, generation, or both components.\n",
    "\n",
    "### Diagram\n",
    "https://docs.smith.langchain.com/assets/images/rag_eval_overview-0d95d78db4d60c2bccbd333f8ba75e60.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfc0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# Grade output schema\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    # Note that the order in the fields are defined is the order in which the model will generate them.\n",
    "    # It is useful to put explanations before responses because it forces the model to think through\n",
    "    # its final response before generating it:\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\n",
    "\n",
    "# Grade prompt\n",
    "correctness_instructions = \"\"\"You are an expert data labeler evaluating model outputs for correctness.\n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the MODEL ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the model answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
    "(2) Ensure that the model answer does not contain any conflicting statements.\n",
    "(3) It is OK if the model answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
    "\n",
    "Correctness:\n",
    "A correctness value of True means that the model's answer meets all of the criteria.\n",
    "A correctness value of False means that the model's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grader_llm = ChatOpenAI(model=\"gpt-4.1-2025-04-14\", temperature=0).with_structured_output(CorrectnessGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
    "    answers = f\"\"\"\\\n",
    "QUESTION: {inputs['question']}\n",
    "GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "MODEL ANSWER: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Run evaluator\n",
    "    grade = grader_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": correctness_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "    return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83444cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class RelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    relevant: Annotated[bool, ..., \"Provide the score on whether the answer addresses the question\"]\n",
    "\n",
    "# Grade prompt\n",
    "relevance_instructions=\"\"\"You are an expert data labeler evaluating model outputs for correctness.\n",
    "\n",
    "You will be given a QUESTION and a MODEL ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the MODEL ANSWER is concise and relevant to the QUESTION\n",
    "(2) Ensure the MODEL ANSWER helps to answer the QUESTION\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the model's answer meets all of the criteria.\n",
    "A relevance value of False means that the model's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "relevance_llm = ChatOpenAI(model=\"gpt-4.1-2025-04-14\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "# Evaluator\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"\n",
    "    answer = f\"QUESTION: {inputs['question']}\\nMODEL ANSWER: {outputs['answer']}\"\n",
    "    grade = relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": relevance_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class GroundedGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    grounded: Annotated[bool, ..., \"Provide the score on if the answer hallucinates from the documents\"]\n",
    "\n",
    "# Grade prompt\n",
    "grounded_instructions = \"\"\"You are an expert data labeler evaluating model outputs for correctness. \n",
    "\n",
    "You will be given FACTS and a MODEL ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the MODEL ANSWER is grounded in the FACTS. \n",
    "(2) Ensure the MODEL ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Grounded:\n",
    "A grounded value of True means that the student's answer meets all of the criteria.\n",
    "A grounded value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM \n",
    "grounded_llm = ChatOpenAI(model=\"gpt-4.1-2025-04-14\", temperature=0).with_structured_output(GroundedGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "# Evaluator\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    answer = f\"FACTS: {doc_string}\\nMODEL ANSWER: {outputs['answer']}\"\n",
    "    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\": \"user\", \"content\": answer}])\n",
    "    return grade[\"grounded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff73c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class RetrievalRelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    relevant: Annotated[bool, ..., \"True if the retrieved documents are relevant to the question, False otherwise\"]\n",
    "\n",
    "# Grade prompt\n",
    "retrieval_relevance_instructions = \"\"\"You are an expert data labeler evaluating information retrieval (RAG) outputs for correctness. \n",
    "\n",
    "You will be given a QUESTION and a set of FACTS provided by the RAG MODEL. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\n",
    "(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\n",
    "(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.\n",
    "A relevance value of False means that the FACTS are completely unrelated to the QUESTION.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "retrieval_relevance_llm = ChatOpenAI(model=\"gpt-4.1-2025-04-14\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for document relevance\"\"\"\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    answer = f\"FACTS: {doc_string}\\nQUESTION: {inputs['question']}\"\n",
    "\n",
    "    # Run evaluator\n",
    "    grade = retrieval_relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": retrieval_relevance_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30836f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    "    metadata={\"version\": \"gpt-4.1-nano\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Explore results locally as a dataframe if you have pandas installed\n",
    "experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
