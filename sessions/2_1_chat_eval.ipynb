{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec81a16",
   "metadata": {},
   "source": [
    "# Chat Evaluation for Financial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849e1a6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this workshop, we'll work with a **financial analyst chatbot** that can answer financial analysis questions and process financial data. Our goal is to set up comprehensive evaluations for this agent to measure how well it performs on financial analysis tasks.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "Throughout the following sections, we'll systematically build evaluation capabilities for our financial analysis agent, allowing you to measure performance quickly and reliably so you can iterate with confidence.\n",
    "\n",
    "## Workshop Objectives\n",
    "\n",
    "Using **LangSmith** as our evaluation platform, we will:\n",
    "\n",
    "1. **Create an initial dataset** to measure performance on financial analysis tasks\n",
    "2. **Define metrics** to evaluate the quality of financial insights and style of recommendations  \n",
    "3. **Run evaluations** on different prompts, models, and agent configurations\n",
    "4. **Compare results manually** to understand strengths and weaknesses\n",
    "5. **Track results over time** to monitor improvements and regressions\n",
    "\n",
    "By the end of this workshop, you'll have an evaluation framework that will help you iterate quickly on developing and enhancing this financial agent.\n",
    "\n",
    "---\n",
    "\n",
    "*Let's get started by examining our financial analyst chatbot and understanding its current capabilities...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d024c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langsmith langchain-openai openai openevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up environment variables with your input\n",
    "print(\"Please enter your API keys to get started:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# LangSmith tracing setting\n",
    "langsmith_tracing = input(\"Enable LangSmith tracing? (true/false) [default: true]: \").strip() or \"true\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = langsmith_tracing\n",
    "\n",
    "# LangSmith API key (secure input)\n",
    "if not os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    langsmith_api_key = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = langsmith_api_key\n",
    "else:\n",
    "    print(\"✓ LangSmith API key already set\")\n",
    "\n",
    "# OpenAI API key (secure input)  \n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    openai_api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "else:\n",
    "    print(\"✓ OpenAI API key already set\")\n",
    "\n",
    "print(\"\\n✓ Environment setup complete!\")\n",
    "print(\"You can now proceed with the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d985473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and initialize langsmith client\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name your dataset and add it to LangSmith\n",
    "dataset_name = \"Financial Analysis Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910f06e",
   "metadata": {},
   "source": [
    "## Creating Your Evaluation Dataset\n",
    "\n",
    "Before testing your financial analysis agent, you need to define the datapoints for evaluation. Here are the key considerations:\n",
    "\n",
    "### Schema Design\n",
    "* **Minimum requirement:** Include inputs to your application (the financial questions)\n",
    "* **Recommended:** Define expected outputs (ideal answers your agent should provide)\n",
    "* **Advanced:** Add additional context like expected data sources or reasoning steps\n",
    "* **Flexibility:** LangSmith datasets support arbitrary schemas as your needs evolve\n",
    "\n",
    "### Dataset Size\n",
    "* **Start small:** Even 10-50 examples provide significant value\n",
    "* **Focus on coverage:** Ensure you capture edge cases and scenarios you want to guard against\n",
    "* **Grow over time:** Datasets are living constructs that expand as you learn from real usage\n",
    "\n",
    "### Data Collection Strategy\n",
    "* **Begin manually:** Hand-label your first 10-20 examples to establish quality baselines\n",
    "* **Learn from users:** Add problematic real-world examples as you discover pain points\n",
    "* **Iterate continuously:** Evaluation is an ongoing process, not a one-time setup\n",
    "* **Consider synthesis:** Advanced teams can augment with synthetically generated data\n",
    "\n",
    "### Our Approach\n",
    "For this tutorial, we'll create 5 financial analysis datapoints focusing on NVIDIA and AMD. Each example includes a question input and expected answer output for our question-answering agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77576b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your dataset examples\n",
    "client.create_examples(\n",
    "\tdataset_id=dataset.id,\n",
    "\texamples=[\n",
    "\t\t {\n",
    "\t\t\"inputs\": {\"question\": \"What is NVIDIA's primary revenue driver?\"},\n",
    "\t\t\"outputs\": {\"answer\": \"Primarily data center chips and AI accelerators\"},\n",
    "\t\t },\n",
    "\t\t {\n",
    "\t\t\"inputs\": {\"question\": \"What market does AMD compete in?\"},\n",
    "\t\t\"outputs\": {\"answer\": \"CPUs, GPUs, and data center processors\"},\n",
    "\t\t },\n",
    "\t\t {\n",
    "\t\t\"inputs\": {\"question\": \"Who leads the AI chip market?\"},\n",
    "\t\t\"outputs\": {\"answer\": \"NVIDIA dominates with over 80% market share\"},\n",
    "\t\t },\n",
    "\t\t {\n",
    "\t\t\"inputs\": {\"question\": \"What is AMD's main competitive advantage?\"},\n",
    "\t\t\"outputs\": {\"answer\": \"Cost-effective alternatives to Intel and NVIDIA\"},\n",
    "\t\t },\n",
    "\t\t {\n",
    "\t\t\"inputs\": {\"question\": \"Does NVIDIA or AMD have higher gross margins?\"},\n",
    "\t\t\"outputs\": {\"answer\": \"NVIDIA typically maintains higher gross margins\"},\n",
    "\t\t }\n",
    " \t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19193c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use predefined scoring prompts (below) or create our own\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT, CONCISENESS_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd39fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the predefined scoring prompts\n",
    "print(\"CORRECTNESS_PROMPT\")\n",
    "print(CORRECTNESS_PROMPT)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"CONCISENESS_PROMPT\")\n",
    "print(CONCISENESS_PROMPT)\n",
    "\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using predefined scoring prompts\n",
    "# def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "#     evaluator = create_llm_as_judge(\n",
    "#         prompt=CORRECTNESS_PROMPT,\n",
    "#         model=\"openai:o3-mini\",\n",
    "#         feedback_key=\"correctness\",\n",
    "#     )\n",
    "#     eval_result = evaluator(\n",
    "#         inputs=inputs,\n",
    "#         outputs=outputs,\n",
    "#         reference_outputs=reference_outputs\n",
    "#     )\n",
    "#     return eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86c934",
   "metadata": {},
   "source": [
    "## Defining Evaluation Metrics\n",
    "\n",
    "Now that we have our dataset, we need to establish metrics to measure our financial agent's performance. Here's our evaluation strategy:\n",
    "\n",
    "### Evaluation Challenges\n",
    "* **Semantic similarity:** We don't expect exact word matches with reference answers\n",
    "* **Contextual correctness:** Financial responses need accuracy, not just similar phrasing  \n",
    "* **Multiple valid answers:** Financial questions often have several correct approaches\n",
    "\n",
    "### Our Two-Metric Approach\n",
    "\n",
    "#### 1. Correctness Evaluation\n",
    "* **Method:** LLM-as-a-judge using GPT-4o-mini\n",
    "* **Why LLM:** Too complex for simple string matching or rule-based functions\n",
    "* **Rubric criteria:** \n",
    "  - Accurate and complete information\n",
    "  - No factual errors\n",
    "  - Addresses all question components\n",
    "  - Logical consistency\n",
    "  - Precise financial terminology\n",
    "* **Output:** Binary CORRECT/INCORRECT assessment\n",
    "\n",
    "#### 2. Conciseness Evaluation  \n",
    "* **Method:** Simple Python function measuring response length\n",
    "* **Threshold:** Response must be less than 2x the reference answer length\n",
    "* **Rationale:** Financial analysis should be clear and concise, not verbose\n",
    "* **Output:** Boolean pass/fail metric\n",
    "\n",
    "### Implementation Benefits\n",
    "* **Automated assessment:** Both metrics run without human intervention\n",
    "* **Scalable evaluation:** Can process large numbers of examples quickly\n",
    "* **Balanced measurement:** Captures both accuracy and response quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7055fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-2025-04-14\", temperature=0)\n",
    "\n",
    "eval_instructions = \"\"\"\n",
    "You are an expert data labeler evaluating model outputs for correctness. Your task is to grade the response as CORRECT or INCORRECT based on the following rubric:\n",
    "\n",
    "<Rubric>\n",
    "A correct answer:\n",
    "- Provides accurate and complete information\n",
    "- Contains no factual errors\n",
    "- Addresses all parts of the question\n",
    "- Is logically consistent\n",
    "- Uses precise and accurate terminology\n",
    "</Rubric>\n",
    "\"\"\"\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    user_content = f\"\"\"You are grading the following question:\n",
    "    {inputs['question']}\n",
    "    Here is the real answer:\n",
    "    {reference_outputs['answer']}\n",
    "    You are grading the following predicted answer:\n",
    "    {outputs['response']}\n",
    "    Respond with CORRECT or INCORRECT:\n",
    "    Grade:\n",
    "    \"\"\"\n",
    "    response = llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": eval_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ],\n",
    "    ).content\n",
    "    return response == \"CORRECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d56808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285100b",
   "metadata": {},
   "source": [
    "## Running Evaluations\n",
    "\n",
    "With our dataset and metrics in place, we're ready to evaluate our financial analysis agent. Here's how to set up and execute the evaluation process:\n",
    "\n",
    "### Application Architecture\n",
    "* **Simple design:** System message with instructions + user question passed to LLM\n",
    "* **LangChain implementation:** Using ChatOpenAI for clean LLM interactions\n",
    "* **Fixed configuration:** \n",
    "  - Model: gpt-4o-mini\n",
    "  - Temperature: 0 for consistent responses\n",
    "  - Customizable instructions for different financial analysis scenarios\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### 1. Core Application Function\n",
    "* **Purpose:** Processes financial questions and returns concise answers\n",
    "* **Default behavior:** Short, concise responses for financial clarity\n",
    "* **Instruction flexibility:** Can modify system prompts for different evaluation tests\n",
    "* **Note:** Current implementation uses fixed model (consider making dynamic if needed)\n",
    "\n",
    "#### 2. LangSmith Integration Wrapper\n",
    "* **Input mapping:** Converts dataset question format to application input\n",
    "* **Output formatting:** Transforms application response to expected evaluation format\n",
    "* **Key requirement:** Maps `inputs[\"question\"]` → `{\"response\": answer}`\n",
    "\n",
    "### Evaluation Execution\n",
    "* **Automated process:** `client.evaluate()` runs your app against the entire dataset\n",
    "* **Multi-metric assessment:** Both conciseness and correctness evaluated simultaneously\n",
    "* **Experiment tracking:** Results tagged with \"openai-4o-mini\" prefix for easy identification\n",
    "* **Scalable testing:** Same setup allows comparing different prompts or configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c77236",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_instructions = \"Respond to the users question in a short, concise manner.\"\n",
    "\n",
    "def my_app(question: str, model: str = \"gpt-4.1-nano-2025-04-14\", instructions: str = default_instructions) -> str:\n",
    "    llm = ChatOpenAI(model=model, temperature=1)\n",
    "    return llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb191efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_target(inputs: str) -> dict:\n",
    "    return {\"response\": my_app(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70129510",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = client.evaluate(\n",
    "    ls_target, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[conciseness, correctness], # The evaluators to score the results\n",
    "    experiment_prefix=\"openai-4.1-nano\", # A prefix for your experiment names to easily identify them\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e15d85",
   "metadata": {},
   "source": [
    "### What adjustments can we make to improve the eval?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
